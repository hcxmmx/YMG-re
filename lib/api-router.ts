// APIè·¯ç”±å™¨ - ç»Ÿä¸€ç®¡ç†Geminiå’ŒOpenAIå…¼å®¹ç«¯ç‚¹
import { GeminiService } from './gemini';
import { OpenAIService, createOpenAIService } from './services/openai-service';
import { buildGeminiConfig, GeminiConfig } from './config/gemini-config';
import { buildOpenAIConfig, OpenAIConfig, OPENAI_API_TYPES } from './config/openai-config';
import { apiKeyStorage } from './storage';
import type { Message } from './types';

// APIç±»å‹æšä¸¾
export const API_TYPES = {
  GEMINI: 'gemini',
  OPENAI: 'openai'
} as const;

export type ApiType = typeof API_TYPES[keyof typeof API_TYPES];

// APIé…ç½®æ¥å£
export interface ApiConfiguration {
  type: ApiType;
  gemini?: GeminiConfig;
  openai?: OpenAIConfig;
}

// ç»Ÿä¸€çš„APIå“åº”æ¥å£
export interface ApiResponse {
  content: string;
  finishReason?: string;
  usage?: {
    inputTokens?: number;
    outputTokens?: number;
    totalTokens?: number;
  };
}

// è°ƒè¯•ä¿¡æ¯æ¥å£
export interface UnifiedDebugInfo {
  apiType: ApiType;
  endpoint: string;
  model: string;
  messages: Message[];
  parameters: Record<string, any>;
  timestamp: string;
}

export class ApiRouter {
  private geminiService?: GeminiService;
  private openaiService?: OpenAIService;
  private currentConfig?: ApiConfiguration;

  constructor() {
    // åˆå§‹åŒ–æ—¶ä¸åˆ›å»ºæœåŠ¡ï¼Œç­‰å¾…é…ç½®
  }

  // è®¾ç½®APIé…ç½®
  setConfiguration(config: ApiConfiguration) {
    this.currentConfig = config;
    
    if (config.type === API_TYPES.GEMINI && config.gemini) {
      this.geminiService = new GeminiService(config.gemini.apiKey);
    } else if (config.type === API_TYPES.OPENAI && config.openai) {
      this.openaiService = createOpenAIService(config.openai);
    }
  }

  // å‘é€æ¶ˆæ¯
  async sendMessage(
    messages: Message[],
    onProgress?: (chunk: string) => void,
    signal?: AbortSignal
  ): Promise<string> {
    if (!this.currentConfig) {
      throw new Error('APIé…ç½®æœªè®¾ç½®');
    }

    switch (this.currentConfig.type) {
      case API_TYPES.GEMINI:
        return await this.sendGeminiMessage(messages, onProgress, signal);
      
      case API_TYPES.OPENAI:
        return await this.sendOpenAIMessage(messages, onProgress, signal);
      
      default:
        throw new Error(`ä¸æ”¯æŒçš„APIç±»å‹: ${this.currentConfig.type}`);
    }
  }

  // å‘é€Geminiæ¶ˆæ¯
  private async sendGeminiMessage(
    messages: Message[],
    onProgress?: (chunk: string) => void,
    signal?: AbortSignal
  ): Promise<string> {
    if (!this.geminiService || !this.currentConfig?.gemini) {
      throw new Error('GeminiæœåŠ¡æœªåˆå§‹åŒ–');
    }

    // æå–ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æ¶ˆæ¯
    const systemPrompt = messages.find(m => m.role === 'system')?.content || '';
    const userMessages = messages.filter(m => m.role !== 'system');
    
    // æ„å»ºAPIå‚æ•°
    const apiParams = {
      model: this.currentConfig.gemini.model,
      temperature: this.currentConfig.gemini.temperature,
      maxOutputTokens: this.currentConfig.gemini.maxOutputTokens,
      topK: this.currentConfig.gemini.topK,
      topP: this.currentConfig.gemini.topP,
      safetySettings: this.currentConfig.gemini.safetySettings,
      abortSignal: signal
    };

    if (onProgress) {
      // æµå¼å“åº”
      let fullResponse = '';
      const stream = this.geminiService.generateResponseStream(userMessages, systemPrompt, apiParams);
      
      for await (const chunk of stream) {
        fullResponse += chunk;
        onProgress(chunk);
      }
      
      return fullResponse;
    } else {
      // éæµå¼å“åº”
      return await this.geminiService.generateResponse(userMessages, systemPrompt, apiParams);
    }
  }

  // å‘é€OpenAIæ¶ˆæ¯
  private async sendOpenAIMessage(
    messages: Message[],
    onProgress?: (chunk: string) => void,
    signal?: AbortSignal
  ): Promise<string> {
    if (!this.openaiService) {
      throw new Error('OpenAIæœåŠ¡æœªåˆå§‹åŒ–');
    }

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºOpenAIæ ¼å¼
    const openaiMessages = this.convertToOpenAIFormat(messages);
    
    console.log('ğŸ”„ å‘é€OpenAIæ¶ˆæ¯è°ƒè¯•:', {
      isStreaming: !!onProgress,
      messageCount: openaiMessages.length,
      model: this.currentConfig?.openai?.model,
      baseURL: this.currentConfig?.openai?.baseURL,
      stream: this.currentConfig?.openai?.stream
    });
    
    const response = await this.openaiService.sendChatRequest(
      openaiMessages,
      onProgress,
      signal
    );

    if (typeof response === 'string') {
      return response;
    } else {
      throw new Error('OpenAIè¿”å›äº†æµå¯¹è±¡ï¼Œä½†æœªæä¾›è¿›åº¦å›è°ƒ');
    }
  }

  // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
  private convertToGeminiFormat(messages: Message[]): any[] {
    const geminiMessages: any[] = [];
    
    for (const message of messages) {
      if (message.role === 'system') {
        // Geminiå°†ç³»ç»Ÿæ¶ˆæ¯ä½œä¸ºç”¨æˆ·æ¶ˆæ¯å¤„ç†
        geminiMessages.push({
          role: 'user',
          parts: [{ text: `[System] ${message.content}` }]
        });
      } else {
        geminiMessages.push({
          role: message.role === 'user' ? 'user' : 'model',
          parts: [{ text: message.content }]
        });
      }
    }
    
    return geminiMessages;
  }

  // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºOpenAIæ ¼å¼
  private convertToOpenAIFormat(messages: Message[]): Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }> {
    return messages.map(message => ({
      role: message.role === 'assistant' ? 'assistant' : 
            message.role === 'system' ? 'system' : 'user',
      content: message.content
    }));
  }

  // è·å–è°ƒè¯•ä¿¡æ¯
  getDebugInfo(messages: Message[]): UnifiedDebugInfo {
    if (!this.currentConfig) {
      throw new Error('APIé…ç½®æœªè®¾ç½®');
    }

    const timestamp = new Date().toISOString();

    switch (this.currentConfig.type) {
      case API_TYPES.GEMINI:
        return {
          apiType: API_TYPES.GEMINI,
          endpoint: 'https://generativelanguage.googleapis.com/v1beta/models',
          model: this.currentConfig.gemini?.model || 'gemini-2.5-pro',
          messages,
          parameters: {
            temperature: this.currentConfig.gemini?.temperature,
            maxOutputTokens: this.currentConfig.gemini?.maxOutputTokens,
            topK: this.currentConfig.gemini?.topK,
            topP: this.currentConfig.gemini?.topP
          },
          timestamp
        };
      
      case API_TYPES.OPENAI:
        return {
          apiType: API_TYPES.OPENAI,
          endpoint: this.currentConfig.openai?.baseURL || 'https://api.openai.com/v1',
          model: this.currentConfig.openai?.model || 'gpt-4o-mini',
          messages,
          parameters: {
            temperature: this.currentConfig.openai?.temperature,
            max_tokens: this.currentConfig.openai?.maxTokens,
            top_p: this.currentConfig.openai?.topP,
            frequency_penalty: this.currentConfig.openai?.frequencyPenalty,
            presence_penalty: this.currentConfig.openai?.presencePenalty
          },
          timestamp
        };
      
      default:
        throw new Error(`ä¸æ”¯æŒçš„APIç±»å‹: ${this.currentConfig.type}`);
    }
  }

  // æµ‹è¯•å½“å‰é…ç½®çš„è¿æ¥
  async testConnection(): Promise<{ success: boolean; error?: string }> {
    if (!this.currentConfig) {
      return { success: false, error: 'APIé…ç½®æœªè®¾ç½®' };
    }

    try {
      switch (this.currentConfig.type) {
        case API_TYPES.GEMINI:
          if (!this.geminiService) {
            return { success: false, error: 'GeminiæœåŠ¡æœªåˆå§‹åŒ–' };
          }
          // å‘é€ç®€å•çš„æµ‹è¯•æ¶ˆæ¯
          await this.sendMessage([{ id: 'test', role: 'user', content: 'æµ‹è¯•', timestamp: new Date() }]);
          return { success: true };
        
        case API_TYPES.OPENAI:
          if (!this.openaiService) {
            return { success: false, error: 'OpenAIæœåŠ¡æœªåˆå§‹åŒ–' };
          }
          return await this.openaiService.testConnection();
        
        default:
          return { success: false, error: `ä¸æ”¯æŒçš„APIç±»å‹: ${this.currentConfig.type}` };
      }
    } catch (error) {
      return {
        success: false,
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      };
    }
  }

  // è·å–å½“å‰é…ç½®
  getCurrentConfig(): ApiConfiguration | undefined {
    return this.currentConfig;
  }

  // è·å–å½“å‰APIç±»å‹
  getCurrentApiType(): ApiType | undefined {
    return this.currentConfig?.type;
  }
}

// åˆ›å»ºé»˜è®¤çš„APIè·¯ç”±å™¨å®ä¾‹
export function createApiRouter(): ApiRouter {
  return new ApiRouter();
}

// ä»è®¾ç½®æ„å»ºAPIé…ç½®
export function buildApiConfigFromSettings(settings: any): ApiConfiguration {
  const apiType = settings.apiType || API_TYPES.GEMINI;
  
  if (apiType === API_TYPES.GEMINI) {
    const geminiConfig = buildGeminiConfig(settings.apiKey || '', {
      model: settings.model,
      temperature: settings.temperature,
      maxOutputTokens: settings.maxTokens,
      topK: settings.topK,
      topP: settings.topP
    });
    
    return {
      type: API_TYPES.GEMINI,
      gemini: geminiConfig
    };
  } else {
    const openaiConfig = buildOpenAIConfig({
      apiType: settings.openaiApiType || OPENAI_API_TYPES.OPENAI,
      baseURL: settings.openaiBaseURL,
      apiKey: settings.openaiApiKey,
      model: settings.openaiModel,
      maxTokens: settings.openaiMaxTokens,
      temperature: settings.openaiTemperature,
      topP: settings.openaiTopP,
      frequencyPenalty: settings.openaiFrequencyPenalty,
      presencePenalty: settings.openaiPresencePenalty,
      stream: settings.openaiStream,
      customHeaders: settings.openaiCustomHeaders,
      customParams: settings.openaiCustomParams
    });
    
    return {
      type: API_TYPES.OPENAI,
      openai: openaiConfig
    };
  }
}
